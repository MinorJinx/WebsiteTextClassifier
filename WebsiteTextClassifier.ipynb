{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Final_Project_JeremyReynolds.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbSSuKmoR7c6",
        "colab_type": "text"
      },
      "source": [
        "Code adapted from: https://www.opencodez.com/python/text-classification-using-keras.htm\n",
        "\n",
        "Further info can be found here: https://github.com/MinorJinx/WebsiteTextClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZN3oPUGsG9G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove GoogleColab generated directories\n",
        "!rm -r sample_data/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4u3skEbbJxoI",
        "colab_type": "code",
        "outputId": "4bbc11a5-8f12-453a-eb36-2f0c51639590",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "# Download 20Newsgroups dataset\n",
        "# !wget http://qwone.com/~jason/20Newsgroups/20news-bydate.tar.gz\n",
        "!wget https://github.com/MinorJinx/WebsiteTextClassifier/raw/master/20news-bydate.tar.gz"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-29 23:05:07--  https://github.com/MinorJinx/WebsiteTextClassifier/raw/master/20news-bydate.tar.gz\n",
            "Resolving github.com (github.com)... 192.30.255.112\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/MinorJinx/WebsiteTextClassifier/master/20news-bydate.tar.gz [following]\n",
            "--2019-11-29 23:05:07--  https://raw.githubusercontent.com/MinorJinx/WebsiteTextClassifier/master/20news-bydate.tar.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14464277 (14M) [application/octet-stream]\n",
            "Saving to: ‘20news-bydate.tar.gz’\n",
            "\n",
            "20news-bydate.tar.g 100%[===================>]  13.79M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2019-11-29 23:05:08 (116 MB/s) - ‘20news-bydate.tar.gz’ saved [14464277/14464277]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-godXkO8KVdl",
        "colab_type": "code",
        "outputId": "aebcd3db-2465-42aa-b074-4b7830b7c394",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Uncompress datset\n",
        "!tar -xzf 20news-bydate.tar.gz\n",
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20news-bydate.tar.gz  20news-bydate-test  20news-bydate-train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNtxny7FQnWB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os, pickle\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import load_model, Sequential\n",
        "from keras.layers import Activation, Dense, Dropout\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "import sklearn.datasets as skds\n",
        "from pathlib import Path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPLUNHrgMm1Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Suppresses TensorFlow warnings\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "# For reproducibility\n",
        "np.random.seed(1237)\n",
        "\n",
        "# Source file directory\n",
        "path_train = \"20news-bydate-train\"\n",
        "\n",
        "files_train = skds.load_files(path_train,load_content=False)\n",
        "\n",
        "label_index = files_train.target\n",
        "label_names = files_train.target_names\n",
        "labelled_files = files_train.filenames\n",
        "\n",
        "data_tags = [\"filename\",\"category\",\"news\"]\n",
        "data_list = []\n",
        "\n",
        "# Read and add data from file to a list (*Added cp1252 encoding)\n",
        "i=0\n",
        "for f in labelled_files:\n",
        "    data_list.append((f,label_names[label_index[i]],Path(f).read_text(encoding='cp1252')))\n",
        "    i += 1\n",
        "\n",
        "# We have training data available as dictionary filename, category, data\n",
        "data = pd.DataFrame.from_records(data_list, columns=data_tags)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bL_3S3AoMwGj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Seperate 80% data as training and remaining 20% for test.\n",
        "train_size = int(len(data) * .8)\n",
        " \n",
        "train_posts = data['news'][:train_size]\n",
        "train_tags = data['category'][:train_size]\n",
        "train_files_names = data['filename'][:train_size]\n",
        " \n",
        "test_posts = data['news'][train_size:]\n",
        "test_tags = data['category'][train_size:]\n",
        "test_files_names = data['filename'][train_size:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRJteMcpQwNQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 20 news groups\n",
        "num_labels = 20\n",
        "vocab_size = 15000\n",
        "batch_size = 100\n",
        " \n",
        "# define Tokenizer with Vocab Size\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "tokenizer.fit_on_texts(train_posts)\n",
        " \n",
        "x_train = tokenizer.texts_to_matrix(train_posts, mode='tfidf')\n",
        "x_test = tokenizer.texts_to_matrix(test_posts, mode='tfidf')\n",
        " \n",
        "encoder = LabelBinarizer()\n",
        "encoder.fit(train_tags)\n",
        "y_train = encoder.transform(train_tags)\n",
        "y_test = encoder.transform(test_tags)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ECvQ6BvRBof",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = LabelBinarizer()\n",
        "encoder.fit(train_tags)\n",
        "y_train = encoder.transform(train_tags)\n",
        "y_test = encoder.transform(test_tags)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86AJ8t_QREkl",
        "colab_type": "code",
        "outputId": "878821d3-92bb-4996-f76a-8f1240ad7c2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(512, input_shape=(vocab_size,)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(num_labels))\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        " \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        " \n",
        "# history = model.fit(x_train, y_train,\n",
        "#                     batch_size=batch_size,\n",
        "#                     epochs=30,\n",
        "#                     verbose=1,\n",
        "#                     validation_split=0.1)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_10 (Dense)             (None, 512)               7680512   \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 20)                10260     \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 20)                0         \n",
            "=================================================================\n",
            "Total params: 7,953,428\n",
            "Trainable params: 7,953,428\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 8145 samples, validate on 906 samples\n",
            "Epoch 1/30\n",
            "8145/8145 [==============================] - 17s 2ms/step - loss: 1.1385 - acc: 0.6883 - val_loss: 0.4099 - val_acc: 0.8962\n",
            "Epoch 2/30\n",
            "8145/8145 [==============================] - 21s 3ms/step - loss: 0.1421 - acc: 0.9697 - val_loss: 0.4315 - val_acc: 0.8974\n",
            "Epoch 3/30\n",
            "8145/8145 [==============================] - 17s 2ms/step - loss: 0.0748 - acc: 0.9875 - val_loss: 0.4270 - val_acc: 0.8985\n",
            "Epoch 4/30\n",
            "8145/8145 [==============================] - 16s 2ms/step - loss: 0.0563 - acc: 0.9909 - val_loss: 0.4776 - val_acc: 0.8863\n",
            "Epoch 5/30\n",
            "8145/8145 [==============================] - 17s 2ms/step - loss: 0.0348 - acc: 0.9957 - val_loss: 0.4370 - val_acc: 0.9018\n",
            "Epoch 6/30\n",
            "8145/8145 [==============================] - 16s 2ms/step - loss: 0.0723 - acc: 0.9907 - val_loss: 0.5870 - val_acc: 0.8786\n",
            "Epoch 7/30\n",
            "8145/8145 [==============================] - 16s 2ms/step - loss: 0.0388 - acc: 0.9951 - val_loss: 0.5462 - val_acc: 0.8830\n",
            "Epoch 8/30\n",
            "8145/8145 [==============================] - 17s 2ms/step - loss: 0.0279 - acc: 0.9971 - val_loss: 0.5397 - val_acc: 0.8940\n",
            "Epoch 9/30\n",
            "8145/8145 [==============================] - 17s 2ms/step - loss: 0.0333 - acc: 0.9968 - val_loss: 0.5137 - val_acc: 0.8940\n",
            "Epoch 10/30\n",
            "8145/8145 [==============================] - 16s 2ms/step - loss: 0.0226 - acc: 0.9975 - val_loss: 0.4905 - val_acc: 0.8974\n",
            "Epoch 11/30\n",
            "8145/8145 [==============================] - 16s 2ms/step - loss: 0.0242 - acc: 0.9978 - val_loss: 0.4868 - val_acc: 0.8918\n",
            "Epoch 12/30\n",
            "8145/8145 [==============================] - 16s 2ms/step - loss: 0.0342 - acc: 0.9957 - val_loss: 0.6112 - val_acc: 0.8874\n",
            "Epoch 13/30\n",
            "8145/8145 [==============================] - 16s 2ms/step - loss: 0.0367 - acc: 0.9955 - val_loss: 0.6329 - val_acc: 0.8786\n",
            "Epoch 14/30\n",
            "8145/8145 [==============================] - 16s 2ms/step - loss: 0.0400 - acc: 0.9962 - val_loss: 0.5692 - val_acc: 0.8819\n",
            "Epoch 15/30\n",
            "8145/8145 [==============================] - 16s 2ms/step - loss: 0.0328 - acc: 0.9967 - val_loss: 0.5504 - val_acc: 0.8929\n",
            "Epoch 16/30\n",
            "8145/8145 [==============================] - 16s 2ms/step - loss: 0.0436 - acc: 0.9955 - val_loss: 0.6430 - val_acc: 0.8863\n",
            "Epoch 17/30\n",
            "8145/8145 [==============================] - 16s 2ms/step - loss: 0.0844 - acc: 0.9880 - val_loss: 0.9718 - val_acc: 0.8488\n",
            "Epoch 18/30\n",
            "8145/8145 [==============================] - 16s 2ms/step - loss: 0.1480 - acc: 0.9805 - val_loss: 0.9206 - val_acc: 0.8521\n",
            "Epoch 19/30\n",
            "8145/8145 [==============================] - 16s 2ms/step - loss: 0.1000 - acc: 0.9850 - val_loss: 0.8742 - val_acc: 0.8664\n",
            "Epoch 20/30\n",
            "8145/8145 [==============================] - 16s 2ms/step - loss: 0.0618 - acc: 0.9917 - val_loss: 0.7644 - val_acc: 0.8742\n",
            "Epoch 21/30\n",
            "8145/8145 [==============================] - 16s 2ms/step - loss: 0.0458 - acc: 0.9942 - val_loss: 0.7831 - val_acc: 0.8863\n",
            "Epoch 22/30\n",
            "8145/8145 [==============================] - 16s 2ms/step - loss: 0.0459 - acc: 0.9944 - val_loss: 0.8239 - val_acc: 0.8731\n",
            "Epoch 23/30\n",
            "8145/8145 [==============================] - 16s 2ms/step - loss: 0.0444 - acc: 0.9957 - val_loss: 0.8277 - val_acc: 0.8852\n",
            "Epoch 24/30\n",
            "8145/8145 [==============================] - 16s 2ms/step - loss: 0.0385 - acc: 0.9964 - val_loss: 0.7671 - val_acc: 0.8918\n",
            "Epoch 25/30\n",
            "8145/8145 [==============================] - 17s 2ms/step - loss: 0.0370 - acc: 0.9971 - val_loss: 0.7611 - val_acc: 0.8863\n",
            "Epoch 26/30\n",
            "8145/8145 [==============================] - 16s 2ms/step - loss: 0.0466 - acc: 0.9961 - val_loss: 0.7493 - val_acc: 0.8874\n",
            "Epoch 27/30\n",
            "8145/8145 [==============================] - 17s 2ms/step - loss: 0.0420 - acc: 0.9955 - val_loss: 0.8214 - val_acc: 0.8841\n",
            "Epoch 28/30\n",
            "8145/8145 [==============================] - 16s 2ms/step - loss: 0.0406 - acc: 0.9964 - val_loss: 0.7726 - val_acc: 0.8874\n",
            "Epoch 29/30\n",
            "8145/8145 [==============================] - 16s 2ms/step - loss: 0.0315 - acc: 0.9966 - val_loss: 0.8460 - val_acc: 0.8852\n",
            "Epoch 30/30\n",
            "8145/8145 [==============================] - 16s 2ms/step - loss: 0.0546 - acc: 0.9935 - val_loss: 0.8320 - val_acc: 0.8852\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JioYBMBIURdp",
        "colab_type": "code",
        "outputId": "a0ba0565-afee-4401-a439-13d7588d6d27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "score = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)\n",
        " \n",
        "print('Test accuracy:', score[1], '\\n')\n",
        " \n",
        "text_labels = encoder.classes_\n",
        " \n",
        "for i in range(10):\n",
        "    prediction = model.predict(np.array([x_test[i]]))\n",
        "    predicted_label = text_labels[np.argmax(prediction[0])]\n",
        "    #print(test_files_names.iloc[i])\n",
        "    print('Actual label:' + test_tags.iloc[i])\n",
        "    print(\"Predicted label: \" + predicted_label)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2263/2263 [==============================] - 1s 467us/step\n",
            "Test accuracy: 0.888643392855964 \n",
            "\n",
            "Actual label:alt.atheism\n",
            "Predicted label: alt.atheism\n",
            "Actual label:comp.graphics\n",
            "Predicted label: comp.graphics\n",
            "Actual label:sci.med\n",
            "Predicted label: sci.med\n",
            "Actual label:sci.crypt\n",
            "Predicted label: sci.crypt\n",
            "Actual label:comp.os.ms-windows.misc\n",
            "Predicted label: comp.os.ms-windows.misc\n",
            "Actual label:rec.sport.baseball\n",
            "Predicted label: rec.sport.baseball\n",
            "Actual label:soc.religion.christian\n",
            "Predicted label: sci.med\n",
            "Actual label:comp.graphics\n",
            "Predicted label: comp.graphics\n",
            "Actual label:rec.sport.hockey\n",
            "Predicted label: rec.sport.hockey\n",
            "Actual label:rec.sport.hockey\n",
            "Predicted label: rec.sport.baseball\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92jv-HYdVeOh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creates a HDF5 file 'my_model.h5'\n",
        "# model.model.save('my_model.h5')\n",
        " \n",
        "# Save Tokenizer i.e. Vocabulary\n",
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGo0aqHK77V0",
        "colab_type": "code",
        "outputId": "7ec48971-a684-4e59-9cf9-830d7ebc648a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "!wget https://github.com/MinorJinx/WebsiteTextClassifier/raw/master/my_model.h5"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-29 23:15:09--  https://github.com/MinorJinx/WebsiteTextClassifier/raw/master/my_model.h5\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/MinorJinx/WebsiteTextClassifier/master/my_model.h5 [following]\n",
            "--2019-11-29 23:15:10--  https://raw.githubusercontent.com/MinorJinx/WebsiteTextClassifier/master/my_model.h5\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 95473968 (91M) [application/octet-stream]\n",
            "Saving to: ‘my_model.h5’\n",
            "\n",
            "my_model.h5         100%[===================>]  91.05M   252MB/s    in 0.4s    \n",
            "\n",
            "2019-11-29 23:15:12 (252 MB/s) - ‘my_model.h5’ saved [95473968/95473968]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnLAFB6FWEid",
        "colab_type": "code",
        "outputId": "5de59ef3-0af9-48fd-8178-4d68f3262a42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# load our saved model\n",
        "model = load_model('my_model.h5')\n",
        " \n",
        "# load tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "with open('tokenizer.pickle', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)\n",
        "\n",
        "# List the lables created from training\n",
        "encoder.classes_"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc',\n",
              "       'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware',\n",
              "       'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles',\n",
              "       'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt',\n",
              "       'sci.electronics', 'sci.med', 'sci.space',\n",
              "       'soc.religion.christian', 'talk.politics.guns',\n",
              "       'talk.politics.mideast', 'talk.politics.misc',\n",
              "       'talk.religion.misc'], dtype='<U24')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPMJGiNUaD3_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# These are the labels we stored from our training, in exact order\n",
        "labels = np.array(['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc',\n",
        " 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x',\n",
        " 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball',\n",
        " 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space',\n",
        " 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast',\n",
        " 'talk.politics.misc', 'talk.religion.misc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMLFNvlHBRmY",
        "colab_type": "text"
      },
      "source": [
        "Text websites are found here: http://lite.cnn.io/en\n",
        "\n",
        "Change the website variable below to classify it's content."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcKo30DVZOSW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "website = 'http://lite.cnn.io/en/article/h_d9b1226cbd5978f4859d9fa318ac5883'\n",
        "\n",
        "page = urlopen(website).read()\n",
        "soup = BeautifulSoup(page)\n",
        "body = soup.find_all('p')\n",
        "file = open('websiteText.txt', 'w')\n",
        "for p in body:\n",
        "  file.write(str(p.text))\n",
        "file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUPkNsLAW2-0",
        "colab_type": "code",
        "outputId": "c3b73119-38b3-493b-96c2-433d29413a81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# To test against files from 20newsGroup\n",
        "# test_files = [\"20news-bydate-test/talk.politics.mideast/77250\"]\n",
        "\n",
        "# Array that contains out website text from above\n",
        "test_files = [\"websiteText.txt\"]\n",
        "\n",
        "x_data = []\n",
        "for t_f in test_files:\n",
        "    t_f_data = Path(t_f).read_text()\n",
        "    x_data.append(t_f_data)\n",
        " \n",
        "x_data_series = pd.Series(x_data)\n",
        "x_tokenized = tokenizer.texts_to_matrix(x_data_series, mode='tfidf')\n",
        " \n",
        "i=0\n",
        "for x_t in x_tokenized:\n",
        "    prediction = model.predict(np.array([x_t]))\n",
        "    predicted_label = labels[np.argmax(prediction[0])]\n",
        "    second_guess = labels[np.argpartition(prediction[0], -2)[-2:][0]] # Gets index of second best prediction\n",
        "    third_guess = labels[np.argpartition(prediction[0], -3)[-3:][0]] # Gets index of second best prediction\n",
        "    print(\"File:\\t\\t\", test_files[i])\n",
        "    print(\"Predicted label:\", predicted_label, \"  Confidence:\", np.max(prediction[0]))\n",
        "    print(\"Second Guess:   \", second_guess, \"  Confidence:\", np.partition(prediction[0].flatten(), -2)[-2])\n",
        "    print(\"Third Guess:    \", third_guess, \"  Confidence:\", np.partition(prediction[0].flatten(), -3)[-3])\n",
        "    i += 1"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File:\t\t websiteText.txt\n",
            "Predicted label: talk.politics.misc   Confidence: 0.98500293\n",
            "Second Guess:    talk.politics.guns   Confidence: 0.014995642\n",
            "Third Guess:     soc.religion.christian   Confidence: 9.215289e-07\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5UY6mAcNZqi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%pycat websiteText.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0kyAEGRNwUE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%pycat 20news-bydate-test/talk.politics.mideast/77250"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}